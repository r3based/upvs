services:
  postgres:
    image: postgres:15
    environment:
      POSTGRES_USER: upvs
      POSTGRES_PASSWORD: upvs
      POSTGRES_DB: upvs
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U upvs"]
      interval: 5s
      timeout: 5s
      retries: 5

  init:
    build:
      context: ..
      dockerfile: apps/api/Dockerfile
    environment:
      DATABASE_URL: ${DATABASE_URL:-postgresql://upvs:upvs@postgres:5432/upvs}
      DATA_RAW_DIR: ${DATA_RAW_DIR:-/app/data/raw}
      DATA_DERIVED_DIR: ${DATA_DERIVED_DIR:-/app/data/derived}
      EMBEDDINGS_PROVIDER: ${EMBEDDINGS_PROVIDER:-st}
      EMBEDDINGS_MODEL: ${EMBEDDINGS_MODEL:-sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2}
      HF_HOME: ${HF_HOME:-/app/.cache/huggingface}
    volumes:
      - ../data:/app/data
      - ../scripts:/app/scripts
      - huggingface_cache:/app/.cache/huggingface
    dns:
      - 8.8.8.8
      - 8.8.4.4
    command: python /app/scripts/init_data.py
    depends_on:
      postgres:
        condition: service_healthy
    restart: "no"

  api:
    build:
      context: ..
      dockerfile: apps/api/Dockerfile
    environment:
      DATABASE_URL: ${DATABASE_URL:-postgresql://upvs:upvs@postgres:5432/upvs}
      DATA_RAW_DIR: ${DATA_RAW_DIR:-/app/data/raw}
      DATA_DERIVED_DIR: ${DATA_DERIVED_DIR:-/app/data/derived}
      FAISS_INDEX_PATH: ${FAISS_INDEX_PATH:-/app/data/derived/faiss/index.faiss}
      FAISS_MAP_PATH: ${FAISS_MAP_PATH:-/app/data/derived/faiss/id_map.jsonl}
      EMBEDDINGS_PROVIDER: ${EMBEDDINGS_PROVIDER:-st}
      EMBEDDINGS_MODEL: ${EMBEDDINGS_MODEL:-sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2}
      VLLM_URL: ${VLLM_URL:-http://vllm:8000/v1}
      VLLM_MODEL: ${VLLM_MODEL:-Qwen/Qwen2-1.5B-Instruct}
      VLLM_API_KEY: ${VLLM_API_KEY:-EMPTY}
      HF_HOME: ${HF_HOME:-/app/.cache/huggingface}
    volumes:
      - ../data:/app/data
      - huggingface_cache:/app/.cache/huggingface
    ports:
      - "8000:8000"
    depends_on:
      - postgres
      - init
    # vLLM опциональный, поэтому не добавляем в depends_on
    # но если vLLM запущен, он будет доступен по имени сервиса

  web:
    build:
      context: ..
      dockerfile: apps/web/Dockerfile
    environment:
      NEXT_PUBLIC_API_BASE: ${NEXT_PUBLIC_API_BASE:-http://localhost:8000}
      NEXT_PUBLIC_MODE: ${NEXT_PUBLIC_MODE:-api}
    volumes:
      - ../data:/app/data:ro
    ports:
      - "3000:3000"
    depends_on:
      - api

  # Включите при наличии образа vLLM и GPU
  vllm:
    image: vllm/vllm-openai:latest
    command: ["--model", "${VLLM_MODEL:-Qwen/Qwen2-1.5B-Instruct}"]
    environment:
      - VLLM_API_KEY=EMPTY
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0
      - HF_HOME=/root/.cache/huggingface
    ports:
      - "8001:8000"
    volumes:
      - huggingface_cache:/root/.cache/huggingface
    dns:
      - 8.8.8.8
      - 8.8.4.4
    # Используем только deploy.resources для новых версий Docker Compose
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

volumes:
  postgres_data:
  huggingface_cache:
